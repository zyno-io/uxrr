# Live Session Architecture: Evaluation & Rebuild Plan

## Part 1: Root Cause Analysis

After tracing every line of code from `rrweb record()` through the client SDK, server relay, and UI replayer, I've identified the following interacting issues:

### Bug 1: Playback freezes after idle (mouse continues, DOM stops)

**Root cause: `goto()` in live mode disrupts rrweb's event processing timer.**

In `ReplayPlayer.vue:135-141`, after mounting the player in live mode, the code calls:
```typescript
player.goto(latestOffsetMs, true);
```

The intent is to jump to the latest event so the frame renders immediately. But rrweb-player's `goto()` internally calls `pause()` then fast-forwards then `play()`. In live mode, this disrupts the replayer's live event processing timer. After `goto()`, newly added events via `addEvent()` accumulate in the queue but the timer that processes them is broken.

Mouse movements continue to render because rrweb's mouse trail uses a separate canvas overlay rendering path that doesn't depend on the same timer as DOM mutation application.

**Contributing factor: Timestamp compression creates a fragile time relationship.**

`ReplayPlayer.vue:84-95` shifts all mount events so the first event's timestamp = `Date.now()` (agent time). Subsequent events from the client use the CLIENT's `Date.now()`. This creates a dependency on clock synchronization between client and agent machines. Any skew causes events to be scheduled at the wrong time in the replayer's internal queue.

### Bug 2: Share links appear black

**Root cause: Shared viewers can't recover from missing snapshots.**

When a shared viewer connects, the flow is:
1. Server sends `client_connected` to shared viewer
2. Server sends `request_snapshot` to client
3. Client generates snapshot → server relays to all agents (including shared viewer)

But in `useSessionDetail.ts:357`, `onClientConnected()` computes:
```typescript
const wasDisconnected = !clientConnected.value || !isLive.value;
```

On first connect, `clientConnected` is `false`, so `wasDisconnected = true`. This triggers the "reconnection" path which sets `deferLiveMountUntilNextTick = true`. After `nextTick()`, if the snapshot hasn't arrived yet:
```typescript
liveStream?.send({ type: 'request_snapshot' });
```

But for shared viewers, `send()` is a **no-op** (`live-stream.ts:240-242`). The shared viewer can never request a snapshot itself. If the snapshot was lost or arrived during the defer window, the viewer is stuck forever with a black screen. Events continue arriving and mouse movements render via rrweb's overlay, but the DOM is never built because no full snapshot is ever mounted.

**Additionally**, there is no server-side snapshot caching. The snapshot is ephemeral — it's generated by the client SDK and relayed through the server to whoever happens to be connected at that moment. If a viewer connects after the snapshot was already relayed, it gets nothing.

### Bug 3: Client reconnection — DOM renders but no updates

**Root cause: Race condition during async player remount.**

On client reconnection, `useSessionDetail.ts:361` resets `playerHasTimeline = false`, which forces `tryMountBufferedLiveEvents()` to call `mountPlayer()` → `playerRef.value?.mount(events)`. The `mount()` function in `ReplayPlayer.vue` is async:

```typescript
async function mount(events) {
    destroyPlayer();          // ← player = null
    await import('rrweb-player');  // ← yields to microtask queue
    // ... create player ...
    player = new Player(...);
}
```

Between `destroyPlayer()` (player = null) and the player being reconstructed, `livePlayerReady` is already `true` (set in `tryMountBufferedLiveEvents:159`). New events arriving from the WebSocket go to:
```typescript
playerRef.value?.addEvent(event);  // ← player is null → silently returns
```

Events are silently dropped during this window. The initial snapshot renders because it was passed to the `mount()` call directly. But every subsequent incremental event that arrives during the ~1-2 frame async gap is lost. Since DOM mutations build on each other (each mutation references nodes from previous mutations), losing even one mutation can break all subsequent mutations — the replayer's internal DOM diverges from the recorded DOM and never recovers.

### Systemic Issues

Beyond the specific bugs, there are architectural problems:

1. **Complex boolean state machine**: Five interacting boolean flags (`livePlayerReady`, `playerHasTimeline`, `clientConnected`, `deferLiveMountUntilNextTick`, `clientEverConnected`) create a state space of 32 combinations. Edge cases are inevitable.

2. **Snapshot stripping is dangerous** (`useSessionDetail.ts:314-317`): Filtering out type 2 (FullSnapshot) and type 4 (Meta) events from batches prevents rrweb checkpoints from refreshing the replayer's DOM. Over time (past 120s when `checkoutEveryNms` fires), the replayer's DOM can drift from reality since checkpoints that would correct it are stripped.

3. **No event sequencing**: Events have no sequence numbers. There's no way to detect gaps, reorder, or deduplicate. If a WebSocket message is lost or arrives out of order, there's no recovery mechanism.

4. **Server-side logic is interleaved with transport concerns**: `LiveSessionService` directly manages WebSocket connections, Redis pub/sub, buffer persistence, controller election, and message routing. `LiveMessageRelay` handles both message routing logic AND Redis delivery mechanics. This coupling makes it hard to reason about correctness.

5. **No snapshot caching**: The server doesn't cache the latest full snapshot. Every new viewer must trigger a fresh `request_snapshot` round-trip to the client SDK, adding latency and creating windows where the snapshot can be missed.

---

## Part 2: Proposed Architecture

### Design Principles

1. **Pod-agnostic core**: The class that manages session state and message routing knows nothing about pods, Redis, or WebSocket. It emits messages through a transport interface.
2. **Never drop events**: All events go through a queue that survives player lifecycle transitions.
3. **Respect rrweb's design**: Don't fight rrweb with timestamp manipulation or `goto()` hacks. Use its live mode as intended.
4. **Explicit state machine**: Replace boolean flags with a well-defined state machine with named states and explicit transitions.
5. **Server-cached snapshots**: The server caches the latest full snapshot so new viewers get it immediately.

### Server-Side Architecture

```
┌────────────────────────────────────────────────────────┐
│                    WebSocket Layer                       │
│                                                          │
│  ┌──────────┐  ┌───────────┐  ┌─────────────────────┐  │
│  │ Client   │  │  Agent    │  │  Shared Viewer      │  │
│  │   WS     │  │   WS      │  │      WS             │  │
│  └────┬─────┘  └─────┬─────┘  └──────────┬──────────┘  │
│       │               │                    │             │
│  ┌────▼───────────────▼────────────────────▼──────────┐ │
│  │         LiveConnectionManager                       │ │
│  │  - maps WS ↔ session                               │ │
│  │  - manages ping/pong                                │ │
│  │  - parses/serializes messages                       │ │
│  │  - handles WS lifecycle (open/close/error)          │ │
│  └──────────────────────┬─────────────────────────────┘ │
│                         │                                │
│  ┌──────────────────────▼─────────────────────────────┐ │
│  │          LiveSessionStream (POD-AGNOSTIC)           │ │
│  │                                                      │ │
│  │  Pure business logic. No WS, no Redis, no network.   │ │
│  │                                                      │ │
│  │  - Routes client messages → agents                   │ │
│  │  - Routes agent messages → client + other agents     │ │
│  │  - Controller election                               │ │
│  │  - Caches latest full snapshot                       │ │
│  │  - Buffers events/logs for persistence flush         │ │
│  │  - Rate limiting                                     │ │
│  │  - Emits messages via IMessageSink interface         │ │
│  └──────────────────────┬─────────────────────────────┘ │
│                         │                                │
│  ┌──────────────────────▼─────────────────────────────┐ │
│  │          IMessageSink → PodAwareTransport           │ │
│  │                                                      │ │
│  │  Decides HOW to deliver each message:                │ │
│  │  - Target is local? → direct WS send                 │ │
│  │  - Target has remote interest? → Redis publish       │ │
│  │  - Both? → local send + Redis publish                │ │
│  │                                                      │ │
│  │  Also handles:                                       │ │
│  │  - Redis subscription lifecycle                      │ │
│  │  - Pod presence (heartbeat, registration)            │ │
│  │  - Remote agent sync                                 │ │
│  └──────────────────────┬─────────────────────────────┘ │
│                         │                                │
│  ┌──────────────────────▼─────────────────────────────┐ │
│  │          BufferPersistence (unchanged)               │ │
│  │  - flush events → S3                                │ │
│  │  - flush logs → Loki                                │ │
│  │  - flush chat → S3 + Postgres flag                  │ │
│  └─────────────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────────────┘
```

#### `LiveSessionStream` — The Pod-Agnostic Core

This is the single class responsible for producing the stream of messages. It has **zero knowledge** of pods, Redis, WebSocket, or network topology.

```typescript
interface IMessageSink {
    sendToClient(sessionId: string, msg: LiveMessage): void;
    sendToAgent(sessionId: string, agentId: string, msg: LiveMessage): void;
    broadcastToAgents(sessionId: string, msg: LiveMessage, excludeAgentId?: string): void;
}

interface IBufferPersistence {
    flushEvents(sessionId: string, events: unknown[]): void;
    flushLogs(sessionId: string, logs: IngestLogEntry[]): void;
    recordChat(sessionId: string, chat: IChatMessage): void;
}

class LiveSessionStream {
    private sink: IMessageSink;
    private persistence: IBufferPersistence;

    // Per-session state (could be a Map<string, SessionState>)
    // SessionState contains:
    //   clientConnected: boolean
    //   agents: Map<string, AgentInfo>
    //   controllerId?: string
    //   cachedSnapshot?: unknown[]     // <-- KEY ADDITION
    //   pendingEvents: unknown[]
    //   pendingLogs: IngestLogEntry[]
    //   chatMessages: IChatMessage[]

    // ── Client events ──

    onClientConnected(sessionId: string, appId: string): void {
        // 1. Mark client connected
        // 2. Broadcast { type: 'client_connected' } to all agents
        // 3. Request snapshot from client (server sends request_snapshot)
    }

    onClientDisconnected(sessionId: string): void {
        // 1. Mark client disconnected
        // 2. Flush buffers
        // 3. Broadcast { type: 'client_disconnected' } to all agents
    }

    onClientMessage(sessionId: string, msg: LiveMessage): void {
        // 1. Rate limit check
        // 2. If events: buffer for persistence, check flush threshold
        //    If events contain a full snapshot (type 2): cache it as latestSnapshot
        // 3. If logs: buffer for persistence, check flush threshold
        // 4. If chat: record
        // 5. Relay to all agents via sink.broadcastToAgents()
    }

    // ── Agent events ──

    onAgentConnected(sessionId: string, agentId: string, info: AgentInfo): void {
        // 1. Add to agents map
        // 2. Elect controller if none
        // 3. Send control_granted/control_revoked to new agent
        // 4. If client connected: send client_connected to agent
        // 5. If cached snapshot exists: send cached snapshot to agent immediately
        //    (No need to request_snapshot from client — use the cache!)
        // 6. If no cached snapshot and client connected: request_snapshot from client
        // 7. Broadcast updated agent list
    }

    onAgentDisconnected(sessionId: string, agentId: string): void {
        // 1. Remove from agents map
        // 2. Transfer controller if needed
        // 3. If last agent: send agent_disconnected to client
        // 4. Broadcast updated agent list
    }

    onAgentMessage(sessionId: string, agentId: string, msg: LiveMessage): void {
        // 1. Rate limit check
        // 2. Handle take_control (any agent)
        // 3. For all other messages: only if controller
        // 4. Route to client via sink
        // 5. For shared messages (chat, pen, etc.): also broadcast to other agents
    }

    // ── Periodic maintenance ──

    flushBuffers(sessionId: string): void {
        // Called by external timer (e.g., every 5s)
        // Drains pendingEvents → persistence.flushEvents()
        // Drains pendingLogs → persistence.flushLogs()
    }
}
```

**Key behavior change: Snapshot caching.** When the client sends events containing a full snapshot (type 2), the server extracts and caches it (along with the preceding Meta event). When a new agent or shared viewer connects, the server immediately sends the cached snapshot — no need to request a fresh one from the client. This:
- Eliminates the round-trip latency for new viewers
- Ensures shared viewers always get a snapshot (no dependency on `send()`)
- Removes the race condition between `client_connected` and snapshot delivery

#### `PodAwareTransport` implements `IMessageSink`

```typescript
class PodAwareTransport implements IMessageSink {
    // Holds references to actual WebSocket connections
    // Holds Redis pub/sub client
    // Holds PodPresenceService

    sendToClient(sessionId: string, msg: LiveMessage): void {
        const localWs = this.getClientWs(sessionId);
        if (localWs?.readyState === OPEN) {
            localWs.send(JSON.stringify(msg));
        } else if (this.presence.hasRemoteInterest(sessionId)) {
            this.redis.publish(channel, { kind: 'relay', target: 'client', message: msg });
        }
    }

    sendToAgent(sessionId: string, agentId: string, msg: LiveMessage): void {
        const localAgent = this.getAgentWs(sessionId, agentId);
        if (localAgent?.readyState === OPEN) {
            localAgent.send(JSON.stringify(msg));
        } else if (this.presence.hasRemoteInterest(sessionId)) {
            this.redis.publish(channel, { kind: 'relay', target: 'agent', agentId, message: msg });
        }
    }

    broadcastToAgents(sessionId: string, msg: LiveMessage, excludeId?: string): void {
        // Send to all local agents
        for (const [id, ws] of this.localAgents(sessionId)) {
            if (id !== excludeId && ws.readyState === OPEN) {
                ws.send(JSON.stringify(msg));
            }
        }
        // Also publish to Redis for remote agents
        if (this.presence.hasRemoteInterest(sessionId)) {
            this.redis.publish(channel, { kind: 'relay', target: 'agents', message: msg, excludeId });
        }
    }
}
```

This is the ONLY class that knows about pods and Redis. The `LiveSessionStream` just calls `sink.broadcastToAgents()` and doesn't care whether that goes to a local WebSocket, Redis, or both.

### Client-Side Architecture (UI)

#### State Machine

Replace the five boolean flags with an explicit state machine:

```
                    ┌─────────────┐
                    │  CONNECTING  │  (agent WS opening)
                    └──────┬──────┘
                           │ WS open
                    ┌──────▼──────┐
                    │   WAITING   │  (waiting for client)
                    └──────┬──────┘
                           │ client_connected
                    ┌──────▼──────┐
                    │   SYNCING   │  (waiting for full snapshot)
                    └──────┬──────┘
                           │ full snapshot received + player mounted
                    ┌──────▼──────┐
              ┌─────│    LIVE     │◄────┐
              │     └─────────────┘     │
              │ client_disconnected     │ full snapshot received
              │                         │ + player remounted
        ┌─────▼───────┐                │
        │ RECONNECTING ├────────────────┘
        └─────┬───────┘
              │ agent WS closed / user navigates away
        ┌─────▼──────┐
        │   ENDED    │
        └────────────┘
```

Each state has:
- **Entry actions**: What happens when entering this state
- **Event handlers**: Which events are valid in this state and what they do
- **Exit actions**: Cleanup when leaving this state

```typescript
type LiveState = 'connecting' | 'waiting' | 'syncing' | 'live' | 'reconnecting' | 'ended';
```

#### `LivePlayerController` — No More Silent Drops

```typescript
class LivePlayerController {
    private player: RRWebPlayer | null = null;
    private eventQueue: eventWithTime[] = [];
    private mountInProgress = false;

    /** Add event — NEVER drops, ALWAYS queues if player isn't ready */
    addEvent(event: eventWithTime): void {
        if (this.player && !this.mountInProgress) {
            this.player.getReplayer().addEvent(event);
        } else {
            this.eventQueue.push(event);
        }
    }

    /** Mount or remount with a full snapshot */
    async mount(snapshotEvents: eventWithTime[], container: HTMLElement): Promise<void> {
        this.mountInProgress = true;

        // Destroy existing player
        if (this.player) {
            this.player.$destroy();
            this.player = null;
        }

        // Async import (cached after first load)
        const { default: Player } = await import('rrweb-player');

        // Any events that arrived during import are in eventQueue

        // Construct player with snapshot events
        // NO timestamp compression. NO goto().
        // Use rrweb's native live mode with a small buffer.
        this.player = new Player({
            target: container,
            props: {
                events: snapshotEvents,
                liveMode: true,
                autoPlay: true,
                showController: false,
                // Let rrweb buffer events for ~500ms to smooth jitter
                // without timestamp hacks
            }
        });

        // Drain queued events into the now-ready player
        const queued = this.eventQueue.splice(0);
        for (const event of queued) {
            this.player.getReplayer().addEvent(event);
        }

        this.mountInProgress = false;
    }

    destroy(): void {
        this.mountInProgress = true; // prevent addEvent during teardown
        this.player?.$destroy();
        this.player = null;
        this.eventQueue = [];
    }
}
```

**Key differences from current code:**
1. **No `goto()`** — events are applied by rrweb's native live mode timer
2. **No timestamp compression** — events keep their original timestamps
3. **No silent drops** — events queue during mount and drain after
4. **Mount is atomic from the consumer's perspective** — `addEvent()` always works, it just queues if needed

#### `useSessionDetail` Simplification

The composable becomes much simpler with the state machine and player controller:

```typescript
function useSessionDetail(options) {
    const state = ref<LiveState>('connecting');
    const playerController = new LivePlayerController();

    // All events go through the controller — never directly to the player
    function onEvents(events: eventWithTime[]) {
        // Check for full snapshot
        const snapshotIndex = events.findIndex(e => e?.type === 2);

        if (snapshotIndex !== -1 && state.value === 'syncing') {
            // First snapshot — mount the player
            const snapshotEvents = extractSnapshotWithMeta(events, snapshotIndex);
            const postSnapshotEvents = events.slice(snapshotIndex + 1)
                .filter(e => e?.type !== 2 && e?.type !== 4);

            playerController.mount(snapshotEvents, container).then(() => {
                state.value = 'live';
            });

            // Post-snapshot events go through addEvent (queued during mount)
            for (const event of postSnapshotEvents) {
                playerController.addEvent(event);
            }
        } else if (snapshotIndex !== -1 && state.value === 'live') {
            // Mid-session snapshot (checkout or reconnect)
            // Option A: Remount cleanly (safest)
            // Option B: Let rrweb handle it via addEvent (works if rrweb handles checkouts)
            // Recommend Option A for reliability:
            const snapshotEvents = extractSnapshotWithMeta(events, snapshotIndex);
            playerController.mount(snapshotEvents, container);

            const postSnapshotEvents = events.slice(snapshotIndex + 1)
                .filter(e => e?.type !== 2 && e?.type !== 4);
            for (const event of postSnapshotEvents) {
                playerController.addEvent(event);
            }
        } else {
            // Normal incremental events
            for (const event of events) {
                playerController.addEvent(event);
            }
        }
    }

    function onClientConnected() {
        if (state.value === 'waiting' || state.value === 'reconnecting') {
            state.value = 'syncing';
            // Server will send cached snapshot or request one from client
            // No need for the agent to request — server handles it
        }
    }

    function onClientDisconnected() {
        if (state.value === 'live' || state.value === 'syncing') {
            state.value = 'reconnecting';
        }
    }
}
```

**What's eliminated:**
- `livePlayerReady` boolean → replaced by state machine (syncing → live)
- `playerHasTimeline` boolean → player controller handles mount/remount internally
- `deferLiveMountUntilNextTick` → unnecessary because event queue prevents drops
- `clientEverConnected` → replaced by state machine (reconnecting state)
- `liveEventBuffer` → replaced by player controller's event queue
- `lastMetaEvent` → extracted from snapshot at mount time

### Snapshot Handling Strategy

The current code strips full snapshots from live event streams. This is wrong. Instead:

**Server-side: Cache and forward.**
- When client sends events containing a FullSnapshot (type 2), extract the Meta (type 4) + FullSnapshot + any immediately following events as the "cached snapshot"
- Store this as `session.cachedSnapshot`
- When a new viewer connects, immediately send the cached snapshot
- Continue forwarding ALL events (including snapshots) to all agents

**Client-side: Embrace snapshots.**
- When the player receives a full snapshot while already live, remount the player with the new snapshot
- The `LivePlayerController.mount()` handles this cleanly — events queue during mount, nothing is dropped
- This keeps the replayer's DOM perfectly in sync with the recorder
- The 2-minute `checkoutEveryNms` snapshots become health checkpoints, not disruptions

### Timestamp Strategy

**Don't modify timestamps.** Period.

rrweb's live mode is designed to handle real-time event streams. The replayer's internal timer processes events based on their timestamp offset from the first event. If events arrive with timestamps close to "now" (which they do, since they're generated in real-time), the replayer applies them in real-time.

The current timestamp compression was added to make the initial snapshot "appear instantly." Instead:
- rrweb-player will apply the initial snapshot events during construction (they're passed as `events` to the constructor)
- The snapshot is rendered immediately as part of construction
- No `goto()` needed — the player starts in the right state

If there's a concern about the replayer "replaying" the initial snapshot in real-time (showing the DOM being built), this doesn't happen because rrweb-player applies all constructor events synchronously before the first render.

---

## Part 3: Implementation Plan

### Phase 1: Fix Critical Bugs (minimal changes)

These are targeted fixes to the existing code to resolve the three bugs immediately, without the full refactor.

#### 1a. Remove `goto()` in live mode
**File:** `packages/ui/src/components/ReplayPlayer.vue`
- Remove lines 133-142 (the `goto()` call)
- The initial events are already applied during player construction
- rrweb-player renders the initial snapshot without needing `goto()`

#### 1b. Add event queuing during mount
**File:** `packages/ui/src/components/ReplayPlayer.vue`
- Add a `pendingEvents: eventWithTime[]` array
- In `addEvent()`: if `player` is null (mount in progress), push to `pendingEvents`
- At end of `mount()`: drain `pendingEvents` into the new player
- This eliminates the race condition

#### 1c. Cache snapshot on server
**File:** `packages/api/src/services/live/live-message-relay.ts`
- In `handleClientMessage()`, when processing `events` messages, scan for type 2 (FullSnapshot)
- If found, extract Meta + FullSnapshot and store on `conn.cachedSnapshot`

**File:** `packages/api/src/services/live-session.service.ts`
- In `connectAgent()` and `connectSharedViewer()`: if `conn.cachedSnapshot` exists, immediately send it to the new viewer instead of requesting a fresh snapshot from the client
- Only send `request_snapshot` to the client if no cached snapshot exists

#### 1d. Stop stripping full snapshots
**File:** `packages/ui/src/composables/useSessionDetail.ts`
- Remove lines 313-318 (the snapshot stripping filter)
- Instead, when a full snapshot arrives while `livePlayerReady` is true, trigger a clean remount via `tryMountBufferedLiveEvents()` (with `playerHasTimeline` handling the remount)

### Phase 2: Server Refactor — Pod-Agnostic Core

#### 2a. Define `IMessageSink` interface
**New file:** `packages/api/src/services/live/interfaces.ts`
- Define `IMessageSink` with `sendToClient`, `sendToAgent`, `broadcastToAgents`
- Define `IBufferPersistence` with `flushEvents`, `flushLogs`, `recordChat`

#### 2b. Extract `LiveSessionStream`
**New file:** `packages/api/src/services/live/live-session-stream.ts`
- Move all routing logic from `LiveMessageRelay.handleClientMessage()` and `handleAgentMessage()` into this class
- Move controller election calls into this class
- Move buffer management (pendingEvents, pendingLogs, flush thresholds) into this class
- Add snapshot caching (`cachedSnapshot` per session)
- This class accepts `IMessageSink` and `IBufferPersistence` in its constructor
- **No WebSocket, no Redis, no network code whatsoever**

#### 2c. Implement `PodAwareTransport`
**Refactor:** `packages/api/src/services/live/live-message-relay.ts` → becomes `PodAwareTransport`
- Implements `IMessageSink`
- Retains all Redis pub/sub logic
- Retains all local WebSocket send logic
- Retains `onLocalConnectionOpened/Closed` for subscription lifecycle
- Retains `handleRedisSessionMessage` for cross-pod relay
- **No message routing logic** — just delivery

#### 2d. Simplify `LiveSessionService`
**Refactor:** `packages/api/src/services/live-session.service.ts`
- Becomes a thin `LiveConnectionManager`
- Manages WebSocket connection lifecycle (open/close/error/ping/pong)
- Maps connections to `LiveSessionStream` calls
- Delegates everything else to `LiveSessionStream`

### Phase 3: Client Refactor — Clean Player Lifecycle

#### 3a. Extract `LivePlayerController`
**New file:** `packages/ui/src/live-player-controller.ts`
- Encapsulates rrweb-player instance management
- Internal event queue with guaranteed delivery
- `mount()` / `addEvent()` / `destroy()` API
- No timestamp manipulation, no `goto()`

#### 3b. Implement state machine in `useSessionDetail`
**Refactor:** `packages/ui/src/composables/useSessionDetail.ts`
- Replace the five boolean flags with `state: Ref<LiveState>`
- Each state transition has explicit entry/exit actions
- `onEvents`, `onClientConnected`, `onClientDisconnected` dispatch based on current state
- The state machine is simple, testable, and debuggable

#### 3c. Remove timestamp compression from ReplayPlayer
**File:** `packages/ui/src/components/ReplayPlayer.vue`
- Remove lines 84-95 (timestamp compression)
- Remove `goto()` call
- The `mount()` function simply passes events to rrweb-player as-is
- rrweb-player handles live mode timing natively

---

## Part 4: Test Strategy

### Existing Test Inventory

The project has three layers of live session tests:

| Layer | Files | What it tests | Depends on internals? |
|-------|-------|---------------|-----------------------|
| Unit | `useSessionDetail.spec.ts` | Composable state machine via mock callbacks | **Yes** — mocks `playerRef.mount`/`addEvent`, asserts on `livePlayerReady` etc. |
| VRT | `session-detail-live.spec.ts` | UI rendering via mocked WebSocket (`mockLiveWebSocket`) | **No** — protocol-level mocks, screenshot comparison |
| E2E | `live-session-reconnect.spec.ts`, `live-session-shared-reconnect.spec.ts` | Full stack: real SDK → real server → real WS → real replay | **No** — asserts on DOM content in rrweb iframe, console panel text |

### E2E Tests Are the Acceptance Criteria

The full E2E tests already assert on the exact behaviors that are currently broken:

- **Bug 1 (freeze after idle)**: `waitForReplayCounterGreaterThan()` after share viewer join — verifies live visual updates continue advancing
- **Bug 2 (black share links)**: `waitForReplayContent(sharedPage)` — verifies shared viewer has rendered content, not black
- **Bug 3 (reconnection)**: `waitForReplayCounterValue(agentPage, 2)` — verifies the rrweb iframe DOM shows the post-reconnect counter value

These tests run three Chromium contexts (client SDK page, agent viewer, shared viewer) against a real API server. They verify end results without coupling to implementation details. **They require zero changes for the rebuild** — they define the behavior we're building toward.

The VRT tests (`mockLiveWebSocket` scripted sequence) and their screenshot comparisons are also unaffected because the WebSocket message format (`{ type: 'events', data: [...] }`, `{ type: 'client_connected' }`, etc.) is unchanged.

### Unit Tests That Need Updating

Two unit tests in `useSessionDetail.spec.ts` explicitly assert the snapshot-stripping behavior being removed:

1. **"ignores full snapshot refreshes while already live" (line 239)** — asserts `mount` is called only once when a mid-session snapshot arrives. New behavior: remount on mid-session snapshots. Update to assert `mount` called twice.

2. **"ignores duplicate full snapshot bursts while already live" (line 296)** — same pattern. Update to assert remount occurs.

The remaining unit tests are correct and survive with minor API adjustments:
- "restarts live mode when client reconnects" — tests state transitions, still valid
- "becomes live again after full snapshot arrives post-reconnect" — tests snapshot gating, still valid
- "re-mounts from latest meta + first full snapshot" — tests buffer extraction, still valid
- "remounts on reconnect snapshot when timeline exists" — tests reconnect remount, still valid
- "buffers pre-connect snapshots and mounts only after client_connected" — tests ordering gate, still valid

### New Tests to Add

#### Server: `LiveSessionStream` unit tests
Since `LiveSessionStream` takes `IMessageSink` and `IBufferPersistence` as interfaces, it can be tested with pure mocks — no WebSocket, no Redis, no network:

- **Message routing**: client events → `sink.broadcastToAgents()` called
- **Controller gating**: non-controller agent messages are dropped
- **Snapshot caching**: client sends full snapshot → `cachedSnapshot` is set; new agent connects → `sink.sendToAgent()` with cached snapshot
- **Rate limiting**: burst of messages → excess silently dropped
- **Buffer flush thresholds**: 50 events → `persistence.flushEvents()` called
- **Agent lifecycle**: first agent → `agent_connected` to client; last agent leaves → `agent_disconnected`

These tests validate all routing logic that currently lives scattered across `LiveMessageRelay` and `LiveSessionService`, now centralized and testable.

#### Client: `LivePlayerController` unit tests
- **Event queuing during mount**: call `mount()` (async), then `addEvent()` during await → events drain after mount completes
- **No silent drops**: verify event count in + event count out always match
- **Remount**: call `mount()` again while live → old player destroyed, new player created, queued events applied
- **Destroy**: call `destroy()` → player null, queue cleared

#### Client: State machine tests
- **Explicit transitions**: verify every valid transition (`waiting → syncing → live → reconnecting → syncing → live → ended`)
- **Invalid transitions**: verify no unexpected state changes (e.g., can't go from `ended` to `live` without reconnect)

### Test Execution Strategy

During implementation, run tests in this order:

1. **Unit tests first** — fast feedback on `LiveSessionStream`, `LivePlayerController`, state machine
2. **VRT tests** — verify UI rendering hasn't regressed (screenshot comparison)
3. **E2E tests last** — full stack integration; these are the final acceptance gate

The E2E tests have generous timeouts (120s, 180s) and built-in retry/stability helpers (`captureStableScreenshot`, `waitForReplayFixtureStable`), making them suitable for CI validation of the rebuild.

---

## Part 5: Implementation Order

Phase 2 (server) and Phase 3 (client) are independent and can proceed in parallel. Within each:

### Phase 2: Server

1. Define `IMessageSink` and `IBufferPersistence` interfaces
2. Extract `LiveSessionStream` with snapshot caching
3. Write unit tests for `LiveSessionStream` against mock interfaces
4. Refactor `LiveMessageRelay` → `PodAwareTransport` implementing `IMessageSink`
5. Thin out `LiveSessionService` to connection lifecycle only
6. Run E2E tests to verify no behavioral regression

### Phase 3: Client

1. Extract `LivePlayerController` with event queue
2. Write unit tests for `LivePlayerController`
3. Refactor `useSessionDetail` to use state machine + `LivePlayerController`
4. Remove timestamp compression and `goto()` from `ReplayPlayer.vue`
5. Update `useSessionDetail.spec.ts` (snapshot-stripping tests → remount tests)
6. Run VRT tests to verify rendering
7. Run E2E tests for full stack validation
